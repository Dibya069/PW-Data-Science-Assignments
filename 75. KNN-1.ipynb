{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. In KNN, the prediction for a new data point is based on the majority class (for classification) or the average (for regression) of its K nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choosing the value of K in KNN is a critical aspect of the algorithm. A small K may lead to noise sensitivity, while a large K may smooth out decision boundaries. The choice of K depends on the specific dataset and problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN Classifier: In classification, KNN assigns the class label based on the majority class among its K nearest neighbors.\n",
    "\n",
    "- KNN Regressor: In regression, KNN predicts the target variable by averaging the values of its K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performance in KNN can be assessed using metrics such as accuracy, precision, recall, F1-score for classification, and metrics like mean squared error (MSE) for regression. Cross-validation is often used to get a more robust estimate of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The curse of dimensionality refers to the problem where the feature space becomes increasingly sparse as the number of dimensions (features) increases. \n",
    "- In KNN, this can lead to a loss of effectiveness as distances between points become more uniform, and the concept of proximity becomes less meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN can be sensitive to missing values. \n",
    "- Common approaches include imputing missing values before applying KNN or using specialized KNN variants that can handle missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN Classifier: Suitable for classification problems where the output is a categorical label. It works well when decision boundaries are not linear and the underlying structure is complex.\n",
    "\n",
    "- KNN Regressor: Suitable for regression problems where the output is a continuous variable. It's effective when the relationship between features and target is not linear.\n",
    "\n",
    "- The choice depends on the nature of the problem and the type of output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strengths:\n",
    "    - Simple to understand and implement.\n",
    "    - Non-parametric, making it flexible for various data distributions.\n",
    "    - Can adapt to complex decision boundaries.\n",
    "\n",
    "- Weaknesses:\n",
    "    - Sensitive to outliers and noisy data.\n",
    "    - Computationally expensive, especially for large datasets.\n",
    "    - Performance can degrade with high-dimensional data.\n",
    "\n",
    "- To address weaknesses, consider preprocessing (e.g., outlier removal, dimensionality reduction), optimizing the choice of K, and using distance-weighted variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Euclidean Distance: It is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding coordinates.\n",
    "\n",
    "- Manhattan Distance: Also known as the L1 norm or Taxicab distance, it is the sum of the absolute differences between corresponding coordinates. It is like measuring the distance traveled along city blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is crucial in KNN because it is a distance-based algorithm. If features are on different scales, the feature with a larger scale can dominate the distance calculation, leading to biased results. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
