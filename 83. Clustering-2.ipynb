{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb1091b",
   "metadata": {},
   "source": [
    "# Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4a3f6",
   "metadata": {},
   "source": [
    "- Hierarchical clustering is a clustering algorithm that groups similar data points into clusters using a hierarchical structure. \n",
    "- Hierarchical clustering is different from other clustering techniques, such as k-means clustering, in that it does not require the number of clusters to be specified beforehand. Instead, hierarchical clustering creates a tree-like structure (dendrogram) that allows the user to visualize and choose the number of clusters based on the structure of the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502a75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c297fb61",
   "metadata": {},
   "source": [
    "# Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8318fbf",
   "metadata": {},
   "source": [
    "- The two main types of hierarchical clustering algorithms are agglomerative clustering and divisive clustering.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Agglomerative clustering is a bottom-up approach where each data point starts as its own cluster, and clusters are merged together based on their similarity. At each step, the two closest clusters are merged until there is only one cluster left. The distance between two clusters is typically defined as the distance between the closest (i.e., most similar) pair of points in each cluster. This process can be visualized as a dendrogram, which is a tree-like structure that shows the hierarchy of clusters at different levels of similarity.\n",
    "\n",
    "Divisive clustering, on the other hand, is a top-down approach where all data points start in a single cluster, and clusters are split apart based on their dissimilarity. At each step, the most dissimilar cluster is split into two until each data point is in its own cluster. The distance between two clusters is typically defined as the distance between the furthest (i.e., most dissimilar) pair of points in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db9978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8763c059",
   "metadata": {},
   "source": [
    "# Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05743e5",
   "metadata": {},
   "source": [
    "Euclidean distance: This is the most commonly used distance metric and measures the straight-line distance between two points in a multi-dimensional space.\n",
    "\n",
    "Manhattan distance: This distance metric measures the distance between two points by summing the absolute differences between their coordinates along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956a976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09e4e6f3",
   "metadata": {},
   "source": [
    "# Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a046371",
   "metadata": {},
   "source": [
    "Dendrogram: One approach is to examine the dendrogram produced by the hierarchical clustering algorithm and visually identify the point at which the branches of the tree merge. This can provide a rough estimate of the number of clusters that are meaningful.\n",
    "\n",
    "Elbow method: This method involves plotting the within-cluster sum of squares (WSS) as a function of the number of clusters and identifying the \"elbow point\" where the rate of reduction in WSS starts to level off. This point indicates the number of clusters that provides the best trade-off between minimizing the distance between the data points within each cluster and maximizing the distance between the clusters.\n",
    "\n",
    "Silhouette method: This method computes a silhouette score for each data point, which measures how well it fits into its assigned cluster relative to other clusters. The average silhouette score for all data points can then be used to evaluate the quality of the clustering for different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85ad64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fddd017e",
   "metadata": {},
   "source": [
    "# Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dbae3",
   "metadata": {},
   "source": [
    "Dendrograms are a visual representation of the results of hierarchical clustering that display the relationships between the data points and the clusters in a tree-like structure. In a dendrogram, each leaf node represents a single data point, and each non-leaf node represents a cluster that is formed by merging the sub-clusters below it. The height of each non-leaf node in the dendrogram represents the distance between the two clusters that are being merged at that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa489c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdbde27",
   "metadata": {},
   "source": [
    "# Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6af961",
   "metadata": {},
   "source": [
    "Yes, hierarchical clustering can be used for both numerical and categorical data. However, the distance metrics used to measure the similarity or dissimilarity between data points differ depending on the type of data.\n",
    "\n",
    "For numerical data, commonly used distance metrics include:\n",
    "\n",
    "Euclidean distance: This is the most commonly used distance metric for numerical data and measures the straight-line distance between two points in n-dimensional space.\n",
    "\n",
    "Manhattan distance: This measures the distance between two points as the sum of the absolute differences between their coordinates.\n",
    "\n",
    "Cosine similarity: This measures the cosine of the angle between two vectors and is often used for text mining and document clustering.\n",
    "\n",
    "For categorical data, commonly used distance metrics include:\n",
    "\n",
    "Jaccard distance: This measures the dissimilarity between two sets of binary attributes and is defined as the ratio of the number of attributes that are different between the two sets to the total number of attributes.\n",
    "\n",
    "Hamming distance: This measures the number of attributes that differ between two categorical variables.\n",
    "\n",
    "Gower distance: This is a generalization of the Jaccard and Hamming distances and can be used for mixed data types, including categorical and numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c242d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce44043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d5a7d86",
   "metadata": {},
   "source": [
    "# Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf77b2f",
   "metadata": {},
   "source": [
    "The first step is to perform hierarchical clustering on your data using an appropriate distance metric and linkage method. Once the dendrogram is constructed, you can visually inspect the tree structure to identify any data points that are significantly far away from their corresponding clusters.\n",
    "\n",
    "Outliers can be identified as data points that appear as singletons (i.e., individual leaf nodes) or are located on long branches that are distant from other branches in the dendrogram. These data points may have high dissimilarity with other data points or may not fit well into any of the clusters formed by the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f9e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
