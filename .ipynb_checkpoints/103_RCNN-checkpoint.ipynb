{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db5150d-64ae-49e7-8d29-f24b7ac9b6d7",
   "metadata": {},
   "source": [
    "## 1. What are the objective of using Selective Search in R-CNN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1317e2-8833-4a0f-89b2-400d2c9a2b4f",
   "metadata": {},
   "source": [
    "- Selective Search is used as a region proposal method in R-CNN (Region-based Convolutional Neural Network) to generate potential regions of interest (ROIs) in an input image.\n",
    "- The objective of using Selective Search in R-CNN is to efficiently propose a diverse set of regions that likely contain objects, helping to focus subsequent computational efforts on the most promising areas of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e0bc2-544b-4210-b243-d0d3f557552c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a049de52-a35d-4399-a335-0469526269e1",
   "metadata": {},
   "source": [
    "## 2. Explain the following phases involved in R-CNN:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66782b9f-68aa-4648-b44d-4a208a326662",
   "metadata": {},
   "source": [
    "###    a. Region Proposal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47e26a-55cc-41f9-a593-1f3440ef6bcb",
   "metadata": {},
   "source": [
    "`In the region proposal phase, potential regions of interest (ROIs) are generated from the input image. These regions are proposed based on selective search or another region proposal method. The goal is to identify candidate regions that may contain objects.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e906a-713a-4525-a62b-0fba81f17ef3",
   "metadata": {},
   "source": [
    "###    b. Warping and resizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ceca1-dbc4-4ce3-bf25-7e2adbb035e0",
   "metadata": {},
   "source": [
    "`The purpose of warping and resizing is to make the regions compatible with the input size expected by the subsequent neural network layers.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f094f7-93a2-4dfe-a90c-c3f68d014c71",
   "metadata": {},
   "source": [
    "###    c. Pre trained CNN architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab5dd8-2a3a-4023-813a-ff441fbac8ad",
   "metadata": {},
   "source": [
    "`The regions of interest, now warped and resized, are passed through a pre-trained CNN (Convolutional Neural Network) architecture. The CNN serves as a feature extractor, capturing relevant features from the proposed regions.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9891680-667d-4639-a7d2-bd468c4c797a",
   "metadata": {},
   "source": [
    "###    d. Pre trained SVM models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd335fb-1342-4722-8181-128f8067cdbe",
   "metadata": {},
   "source": [
    "`Following the CNN feature extraction, a support vector machine (SVM) is trained for each object class. These SVM models are used to classify the extracted features into different classes, determining whether a given region contains an object of interest or not. `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b9b1e-e697-4b03-834e-f40c94cea1cb",
   "metadata": {},
   "source": [
    "###    e. Clean up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3dbd72-a9a1-433e-9afe-e92da720e2ff",
   "metadata": {},
   "source": [
    "`After classification, a post-processing step is performed to eliminate duplicate or highly overlapping bounding box proposals. Non-maximum suppression (NMS) is commonly used to clean up the bounding box proposals, keeping only the most confident predictions.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb1c74-19db-454c-b27a-cfa5e212ba75",
   "metadata": {},
   "source": [
    "###    f. Implementation of bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa179a-be70-4cd3-9061-825405f5b459",
   "metadata": {},
   "source": [
    "`The final step involves implementing bounding boxes around the detected objects based on the refined region proposals and their classifications. `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d709086-cba2-43c8-b550-ac0252e6c698",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53d09e",
   "metadata": {},
   "source": [
    "##  3. What are the possible pre trained CNNs we can use in Pre trained CNN Architecrure ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ccae9d",
   "metadata": {},
   "source": [
    "\n",
    "- VGG (Visual Geometry Group) Networks: VGG16, VGG19\n",
    "- Residual Network (ResNet): ResNet50, ResNet101, ResNet152\n",
    "- GoogLeNet (Inception): InceptionV3\n",
    "- Xception: Xception\n",
    "- DenseNet: DenseNet121, DenseNet169, DenseNet201\n",
    "- AlexNet: AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b4f6c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0839b51b",
   "metadata": {},
   "source": [
    "## 4. How is SVM implemented in the R-CNN framework ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530bd1a",
   "metadata": {},
   "source": [
    "- The region proposals are warped and resized to a fixed size, usually to match the input size expected by a pre-trained Convolutional Neural Network (CNN).\n",
    "- The warped and resized region proposals are fed through a pre-trained CNN, such as VGG, ResNet, or another architecture. \n",
    "- For each class, a separate SVM is trained using the features extracted by the pre-trained CNN. These SVMs are binary classifiers that determine whether a given region proposal belongs to a particular object class or not.\n",
    "\n",
    "- The combination of region proposal, feature extraction using a pre-trained CNN, and classification using **SVMs** allows the R-CNN framework to detect and classify objects in images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b11b3b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728a5e7",
   "metadata": {},
   "source": [
    "## 5. How does Non-maximum Supression work ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717655a",
   "metadata": {},
   "source": [
    "- Sort all the bounding box proposals in descending order based on their confidence scores. The proposal with the highest confidence score comes first.\n",
    "- Start with the bounding box proposal that has the highest confidence score. \n",
    "- Calculate the Intersection over Union (IoU) between the candidate box and all other remaining boxes in the sorted list. IoU is a measure of the overlap between two bounding boxes and is calculated as the area of intersection divided by the area of the union.\n",
    "- Set a predefined IoU threshold (e.g., 0.5). If the IoU between the candidate box and any other box exceeds this threshold, discard the box with the lower confidence score.\n",
    "- Repeat the process by selecting the next highest-scoring box from the sorted list and calculating IoU with the remaining boxes. Again, discard any boxes that exceed the IoU threshold.\n",
    "- Continue this process until all boxes in the sorted list have been considered.\n",
    "- **This cleaning process is called Non-maximum Supression work**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f117a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b97d6a",
   "metadata": {},
   "source": [
    "## 6. How Fast R-CNN is better than R-CNN ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47148b",
   "metadata": {},
   "source": [
    "- R-CNN used a separate algorithm (Selective Search) for region proposals, which was slow and a bottleneck in the system. Fast R-CNN, on the other hand, incorporates the region proposal step into the neural network itself, making it an end-to-end trainable system.\n",
    "- In R-CNN, each region proposal was independently processed by a CNN, resulting in redundant computations for overlapping regions. Fast R-CNN shares the computation for overlapping regions, making the process more efficient.\n",
    "- R-CNN extracts features separately for each region proposal, leading to redundant feature computation. In Fast R-CNN, features are extracted only once for the entire image, and the region of interest (ROI) pooling layer is used to obtain fixed-size feature vectors for each region proposal. \n",
    "- Fast R-CNN allows end-to-end training of the entire system, including the region proposal network (RPN) and the object detection network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6700a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97660c83",
   "metadata": {},
   "source": [
    "## 7. Using mathematical intuition, explain ROI polling in Fast R-CNN ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71395a",
   "metadata": {},
   "source": [
    "- ROI (Region of Interest) pooling is a crucial step in Fast R-CNN that allows the extraction of a fixed-size feature vector from an arbitrary-sized region of the feature map.\n",
    "- Suppose we have a feature map of size `W × H × C (width, height, channels)`.\n",
    "- For a given region proposal, represented by the coordinates `(x,y,w,h)`, where `(x,y)` is the top-left corner, w and h are the width and height, respectively.\n",
    "- Divide the region proposal into a fixed grid. Each grid cell corresponds to a fraction of the original region. Apply max pooling independently in each grid cell.The result is a fixed-size feature map for the region proposal.\n",
    "\n",
    "`ROI pooling=MaxPool({Feature map(x \n",
    "i\n",
    "​\n",
    " ,y \n",
    "i\n",
    "​\n",
    " )∣i=1,2,...,N})`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c517eed",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d2191",
   "metadata": {},
   "source": [
    "## 8. Explain Following Process..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0390230",
   "metadata": {},
   "source": [
    "### a. ROI Projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e05ba1",
   "metadata": {},
   "source": [
    "- ROI projection refers to the mapping of a region proposal from the original image space to the feature map space. \n",
    "- In Fast R-CNN, this is essential because the region proposals are generated based on the original image, but the subsequent object detection is performed on a feature map.\n",
    "- Mathematically, ROI projection involves scaling and mapping the coordinates of the region proposal (x,y,w,h) from the original image space to the corresponding coordinates in the feature map space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5c9b8",
   "metadata": {},
   "source": [
    "### b. ROI polling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6d19b",
   "metadata": {},
   "source": [
    "- ROI pooling is a process in Fast R-CNN that extracts fixed-size feature maps from the feature map corresponding to a region proposal. \n",
    "- This is necessary because region proposals can be of different sizes, and the subsequent fully connected layers require a consistent input size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8da3bf",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261bbe4",
   "metadata": {},
   "source": [
    "## 9. In comparison with R-CNN, why did the object classifier activate function change in Fast R-CNN ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20b5ff",
   "metadata": {},
   "source": [
    "- In R-CNN, the object classifier used softmax activation for object classification. However, in Fast R-CNN, the softmax activation was replaced with a softmax function applied independently for each class, combined with a binary sigmoid activation for the background class.\n",
    "\n",
    "- Multi-Class Classification within Regions:\n",
    "  `In R-CNN, each region proposal was classified independently into one of the classes using a softmax activation. This approach didn't consider the fact that an object might belong to multiple classes simultaneously.`\n",
    "\n",
    "- Binary Background Class Activation:\n",
    "  `Fast R-CNN introduced a binary sigmoid activation for the background class. This allows the model to distinguish between the object classes and the background, treating background as a separate class.`\n",
    "\n",
    "- Efficiency and Training Simplicity:\n",
    "  `The change in the activation function simplifies the training process. It allows for more flexibility in handling multi-class scenarios and improves the efficiency of the training process.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c119f2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c39a1",
   "metadata": {},
   "source": [
    "## 10. What major changes in Faster R-CNN compared to Fast R-CNN ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6677a",
   "metadata": {},
   "source": [
    "Faster R-CNN is an extension of Fast R-CNN that introduces a Region Proposal Network (RPN) to generate region proposals, eliminating the need for an external region proposal method like Selective Search.\n",
    "\n",
    "- Region Proposal Network (RPN):\n",
    "  `The most significant change is the integration of the Region Proposal Network (RPN) into the overall architecture. The RPN generates region proposals based on anchor boxes and is trained simultaneously with the object detection network.`\n",
    "\n",
    "- End-to-End Training:\n",
    "  `Faster R-CNN enables end-to-end training of both the RPN and the object detection network. This unified training approach leads to better optimization and improved overall performance.`\n",
    "\n",
    "- Anchor Boxes:\n",
    "  `Faster R-CNN uses anchor boxes (also called anchor boxes or default boxes) in the RPN to propose potential regions of interest. These anchor boxes are pre-defined bounding boxes of different scales and aspect ratios. The RPN predicts adjustments to these anchor boxes to refine the proposals.`\n",
    "\n",
    "- Shared Convolutional Features:\n",
    "  `The convolutional features are shared between the RPN and the object detection network, reducing redundancy and computational cost.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a1745",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cd4b2",
   "metadata": {},
   "source": [
    "## 11. Explain the concept of Anchore box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d26536",
   "metadata": {},
   "source": [
    "- Anchor boxes, also known as anchor boxes or default boxes, are a crucial concept in object detection models like Faster R-CNN. They are used by the Region Proposal Network (RPN) to generate potential bounding box proposals.\n",
    "- The use of anchor boxes contributes to the efficiency and accuracy of the region proposal process in object detection models like Faster R-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f72bb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeebb1c1",
   "metadata": {},
   "source": [
    "## 12. Implementation of Fast R-CNN using 2017 COCO dataset i.e train dataset, test dataset and valid dataset. Use pre-trained backbone network like, Resnet or VGG for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6011061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf33dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Faster R-CNN architecture\n",
    "def build_faster_rcnn(num_classes):\n",
    "    # Define the backbone network\n",
    "    backbone = InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    # Define the RPN (Region Proposal Network)\n",
    "    rpn_layers = [\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(2, activation='sigmoid', name='rpn_objectness'),  # (objectness score, bbox regression)\n",
    "    ]\n",
    "    rpn = tf.keras.Sequential(rpn_layers, name='rpn')\n",
    "\n",
    "    # Define the ROI heads (Region of Interest)\n",
    "    roi_heads_layers = [\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes + 1, activation='softmax', name='classification'),  # (background, class 1, class 2, ...)\n",
    "        Dense(num_classes * 4, activation='linear'),    # bbox regression\n",
    "    ]\n",
    "    roi_heads = tf.keras.Sequential(roi_heads_layers, name='roi_heads')\n",
    "\n",
    "    # Create the Faster R-CNN model\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    features = backbone(inputs)\n",
    "    rpn_outputs = rpn(features)\n",
    "    roi_outputs = roi_heads(rpn_outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=roi_outputs)\n",
    "    return model, backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ca6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 34s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Define the number of object classes\n",
    "num_classes = 80  # COCO dataset has 80 classes\n",
    "\n",
    "# Build the Faster R-CNN model\n",
    "model, backbone = build_faster_rcnn(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd25830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained weights for the backbone network\n",
    "model.get_layer('inception_v3').set_weights(backbone.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "514df6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4158f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with appropriate loss functions and optimizer\n",
    "model.compile(\n",
    "    loss={\n",
    "        'rpn_objectness': BinaryCrossentropy(),\n",
    "        'classification': CategoricalCrossentropy()\n",
    "    },\n",
    "    optimizer=SGD(learning_rate=0.001, momentum=0.9),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1503f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Obtaining dependency information for pycocotools from https://files.pythonhosted.org/packages/e9/f5/180c8dfe94d031533619dde10495fda81082ed4578260b6924f1cbc7c977/pycocotools-2.0.7-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pycocotools-2.0.7-cp39-cp39-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\mohan\\appdata\\roaming\\python\\python39\\site-packages (from pycocotools) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pycocotools) (1.22.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mohan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\mohan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=2.1.0->pycocotools) (6.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools) (3.8.0)\n",
      "Downloading pycocotools-2.0.7-cp39-cp39-win_amd64.whl (85 kB)\n",
      "   ---------------------------------------- 85.1/85.1 kB 111.5 kB/s eta 0:00:00\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c3c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e27a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_train2017.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m val_annotation_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_val2017.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize COCO instances for training and validation sets\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m coco_train \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_annotation_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m coco_val \u001b[38;5;241m=\u001b[39m COCO(val_annotation_path)\n",
      "File \u001b[1;32mc:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pycocotools\\coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[1;34m(self, annotation_file)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading annotations into memory...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(dataset))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_train2017.json'"
     ]
    }
   ],
   "source": [
    "# Define the path to your COCO dataset annotations\n",
    "train_annotation_path = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_train2017.json\"\n",
    "val_annotation_path = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_val2017.json\"\n",
    "\n",
    "# Initialize COCO instances for training and validation sets\n",
    "coco_train = COCO(train_annotation_path)\n",
    "coco_val = COCO(val_annotation_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77038c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
