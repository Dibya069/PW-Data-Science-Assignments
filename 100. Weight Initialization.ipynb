{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Assess the understanding of weight initialization techniques in artificial neural networks and evaluate the impact of different initialization methods on model performance. Enhance knowledge of weight initialization's role in improving convergence and avoiding vanishing/exploding gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 1: Understanding Weight Initialization\n",
    "#### Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize the weights carefully? Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence? Discuss the concept of variance and how it relates to weight initialization. When is it crucial to consider the variance of weights during initialization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In artificial neural networks, weight initialization is crucial for achieving effective model training and convergence\n",
    "-  Proper weight initialization sets the initial values of the network's weights, influencing how information is propagated through the network during the training process.\n",
    "\n",
    "* Improper weights:\n",
    "* Vanishing Gradients: If weights are initialized too small, gradients may become extremely small during backpropagation, making it challenging for the model to learn from the data effectively.\n",
    "* Exploding Gradients: Conversely, if weights are initialized too large, gradients may become excessively large, causing the model to diverge or oscillate during training.\n",
    "* Symmetry Issues: Improper initialization can lead to symmetric weights, where neurons in the same layer learn similar features. This redundancy hampers the network's ability to capture diverse features from the input data.\n",
    "\n",
    "- Variance refers to the measure of how much values in a set deviate from their mean. \n",
    "- In the context of weight initialization, variance is related to the spread of initial weight values.\n",
    "\n",
    "* Crucial Considerations for Variance:\n",
    "* Avoiding Saturation: If the variance is too high, it can lead to saturation in activation functions, limiting the dynamic range of neurons and impeding the learning process.\n",
    "* Balancing Exploding/Diminishing Effects: Controlling the variance helps balance the effects of exploding and vanishing gradients, promoting stable and effective learning throughout the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 2: Weight Initialization Techniques\n",
    "#### Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use. Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients? Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it. Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation: Zero initialization involves setting all the weights in the neural network to zero during the initialization phase.\n",
    "- Potential Limitations: The main limitation of zero initialization is that it leads to symmetry issues. All neurons in a layer would learn the same features, causing redundancy and hindering the model's capacity to capture diverse patterns in the data.\n",
    "- Appropriate Use: Zero initialization might be appropriate in specific cases, such as when initializing biases in layers with sigmoid or tanh activations, where symmetry is less of an issue.\n",
    "\n",
    "* Explanation: Random initialization sets the weights to random values, typically drawn from a Gaussian distribution or a uniform distribution.\n",
    "* Mitigating Issues: To mitigate saturation or vanishing/exploding gradients, it is common to scale the randomly initialized weights. This scaling factor can be chosen based on the activation function used, helping maintain a balanced range of activations during training.\n",
    "\n",
    "- Explanation: Xavier/Glorot initialization sets the weights by drawing them from a distribution with zero mean and a variance that is inversely proportional to the sum of the number of input and output units in a layer.\n",
    "- Addressing Challenges: Xavier initialization addresses the challenges of improper weight initialization by ensuring that the weights are neither too small (leading to vanishing gradients) nor too large (leading to exploding gradients). It aims to keep the variance of activations roughly the same across different layers.\n",
    "- Underlying Theory: The theory behind Xavier initialization is based on maintaining a stable distribution of activations throughout the network, which facilitates smoother and more effective training.\n",
    "\n",
    "* Explanation: He initialization, also known as MSRA (Mean Squared Root Adjustment), is similar to Xavier but uses a scaling factor that considers only the number of input units. It sets the variance of the weights to  2/n, where n is the number of input units.\n",
    "* Differences from Xavier: He initialization differs from Xavier in that it considers only the number of input units, not the sum of input and output units. This makes it more suitable for activation functions like ReLU.\n",
    "* Preferred Use: He initialization is often preferred when using Rectified Linear Unit (ReLU) activations, as it helps mitigate the issue of dying ReLU units and encourages a more robust learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 3: Applying Weight Initialization\n",
    "#### Implement different weight initialization techniques (zero initialization, random initialization, Xavier initialization, and He initialization) in a neural network using a framework of your choice. Train the model on a suitable dataset and compare the performance of the initialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(weight_initializer):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_initializer = weight_initializer))\n",
    "    model.add(layers.Dense(10, activation='softmax', kernel_initializer = weight_initializer))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize models with different weight initializations\n",
    "zero_initialized_model = build_model(tf.keras.initializers.Zeros())\n",
    "random_initialized_model = build_model(tf.keras.initializers.RandomNormal(mean=0, stddev=1))\n",
    "xavier_initialized_model = build_model(tf.keras.initializers.GlorotNormal())\n",
    "he_initialized_model = build_model(tf.keras.initializers.HeNormal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101770 (397.54 KB)\n",
      "Trainable params: 101770 (397.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101770 (397.54 KB)\n",
      "Trainable params: 101770 (397.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101770 (397.54 KB)\n",
      "Trainable params: 101770 (397.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101770 (397.54 KB)\n",
      "Trainable params: 101770 (397.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(zero_initialized_model.summary())\n",
    "print(random_initialized_model.summary())\n",
    "print(xavier_initialized_model.summary())\n",
    "print(he_initialized_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\dibya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "_________Training for Zero Weight Initialization is Done_________\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "_________Training for Random Weight Initialization is Done_________\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "_________Training for Xavier Weight Initialization is Done_________\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "_________Training for He Weight Initialization is Done_________\n"
     ]
    }
   ],
   "source": [
    "zero_initialized_history = zero_initialized_model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels), verbose=3)\n",
    "print(\"_________Training for Zero Weight Initialization is Done_________\")\n",
    "\n",
    "random_initialized_history = random_initialized_model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels), verbose=3)\n",
    "print(\"_________Training for Random Weight Initialization is Done_________\")\n",
    "\n",
    "xavier_initialized_history = xavier_initialized_model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels), verbose=3)\n",
    "print(\"_________Training for Xavier Weight Initialization is Done_________\")\n",
    "\n",
    "he_initialized_history = he_initialized_model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels), verbose=3)\n",
    "print(\"_________Training for He Weight Initialization is Done_________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3010 - accuracy: 0.1135\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2423 - accuracy: 0.9295\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0771 - accuracy: 0.9771\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0820 - accuracy: 0.9759\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models on the test set\n",
    "zero_initialized_test_loss, zero_initialized_test_acc = zero_initialized_model.evaluate(test_images, test_labels)\n",
    "random_initialized_test_loss, random_initialized_test_acc = random_initialized_model.evaluate(test_images, test_labels)\n",
    "xavier_initialized_test_loss, xavier_initialized_test_acc = xavier_initialized_model.evaluate(test_images, test_labels)\n",
    "he_initialized_test_loss, he_initialized_test_acc = he_initialized_model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Initialization Test Accuracy: 0.11349999904632568\n",
      "Random Initialization Test Accuracy: 0.9294999837875366\n",
      "Xavier Initialization Test Accuracy: 0.9771000146865845\n",
      "He Initialization Test Accuracy: 0.9758999943733215\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of initialized models\n",
    "print(\"Zero Initialization Test Accuracy:\", zero_initialized_test_acc)\n",
    "print(\"Random Initialization Test Accuracy:\", random_initialized_test_acc)\n",
    "print(\"Xavier Initialization Test Accuracy:\", xavier_initialized_test_acc)\n",
    "print(\"He Initialization Test Accuracy:\", he_initialized_test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
