{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd824f94-99df-4285-88ff-124842bf29a5",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a493590-93d1-4bea-bbb7-eafaf50608e2",
   "metadata": {},
   "source": [
    "## TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec11f1-f83e-4516-b127-897185fd7be6",
   "metadata": {},
   "source": [
    "### 1. Describe the purpose and benifits of pooling in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bf9e1-88dd-40c6-ab5a-e72b7843c4ab",
   "metadata": {},
   "source": [
    "Purpose: \n",
    " - The primary purpose of pooling is to help manage the computational complexity of CNNs, increase translation invariance, and improve the network's ability to learn hierarchical features.\n",
    "\n",
    "Benifits:\n",
    " - Efficient dimensionality reduction.\n",
    " - Translation invariance for enhanced robustness.\n",
    " - Feature generalization for better generalization to new data.\n",
    " - Extraction of dominant features for improved noise resistance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72a8fc-2fca-4e90-8457-434c3f54f983",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294178fc-d012-4e15-aa1a-d87ffa891ab0",
   "metadata": {},
   "source": [
    "### 2. Explain the difference between \"Min pooling\" and \"Max pooling\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4905629-4e29-4b87-98f8-ee7e67ff525c",
   "metadata": {},
   "source": [
    "Max Pooling:\n",
    " - In max pooling, a local region (usually a small grid) of the input feature map is divided into cells. The operation selects the maximum value within each cell and uses that value as the representative feature for that region. The key characteristic of max pooling is that it emphasizes the most dominant or significant feature present in that local region. This helps the network capture the most important information and enhance its sensitivity to distinctive features. Max pooling is particularly effective at retaining high-contrast features and edges.\n",
    "\n",
    "Min Pooling:\n",
    " - In min pooling, the operation is similar to max pooling, but it selects the minimum value within each local region's cell as the representative feature. Min pooling is less commonly used than max pooling and might not be as effective in retaining important features, especially in cases where the maximum values are more indicative of feature presence. Min pooling tends to emphasize low-intensity features, which might not be as useful for tasks like object recognition or edge detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3afc7-8eb7-425a-9716-f985f4640f66",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0448a0-615f-4319-bc8d-8d50d6e1373a",
   "metadata": {},
   "source": [
    "### 3. Discuss the concept of padding in CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3ffc3-ddf2-4c0a-aa08-8fc86a0c4f89",
   "metadata": {},
   "source": [
    "Padding is a technique used in Convolutional Neural Networks (CNNs) to control the spatial dimensions of feature maps as they are processed through convolutional layers. It involves adding extra pixels or values around the edges of an input image or feature map before applying convolutional operations. The main purpose of padding is to preserve spatial information, mitigate the reduction in feature map dimensions, and ensure that the output dimensions match the desired dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404eb34-f537-4902-bfeb-d296e6e79547",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05933fd-b787-45cd-8d0b-e46e980eaef0",
   "metadata": {},
   "source": [
    "### 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the  output feature map size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd501a1-c22e-4d4e-8033-116335a0b4bd",
   "metadata": {},
   "source": [
    "Zero-padding:\n",
    " - Effect on Output Size: Zero-padding increases the size of the output feature map compared to the input size.\n",
    " - Purpose: The primary purpose of zero-padding is to maintain the spatial dimensions of the input and output feature maps. It helps to preserve information at the edges of the input and prevents the reduction of feature map dimensions caused by convolutions.\n",
    " - Use Cases: Zero-padding is often used when you want to maintain spatial information, ensure that the output dimensions match the input dimensions, and when you need to process features near the edges of the input.\n",
    "\n",
    "Valid-padding:\n",
    " - Effect on Output Size: Valid-padding reduces the size of the output feature map compared to the input size.\n",
    " - Purpose: The purpose of valid-padding is to perform dimensionality reduction by allowing the convolutional operation to be applied only within the central regions of the input. It helps to extract key features while reducing computational complexity and output dimensions.\n",
    " - Use Cases: Valid-padding is useful when you want to extract features from an input while reducing its spatial dimensions, which is often desirable in tasks like feature extraction and dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6adaf-c780-4b16-8818-0f5a542295ba",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "\n",
    "- Output Size: Zero-padding increases the output size, while valid-padding reduces it.\n",
    "- Spatial Information: Zero-padding preserves spatial information at the edges, while valid-padding can result in the loss of information at the edges.\n",
    "- Feature Extraction: Zero-padding is suitable for maintaining edge features and preserving spatial relationships between pixels. Valid-padding is effective for reducing dimensions and focusing on central features.\n",
    "- Receptive Field: Zero-padding ensures that more context is considered by each neuron during convolution due to the larger input size. Valid-padding results in a smaller receptive field for each neuron.\n",
    "- Computation: Zero-padding leads to increased computational complexity compared to valid-padding due to the larger input size.\n",
    "- Applications: Zero-padding is often used for tasks like object recognition, image segmentation, and preserving spatial details. Valid-padding is used for tasks like downsampling, dimensionality reduction, and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b95c42-aa1d-490b-a404-257dc92e053e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d857528-4516-4f21-8ada-666c3e7e9e5a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5641680b-dae5-4cab-afb2-1f65651e0f26",
   "metadata": {},
   "source": [
    "\n",
    "## TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa39ac-8cff-4ed8-bc71-1af6e2268dba",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Provide a brief overview of LetNet-5 Architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcec653-2d53-45b2-8bb4-1386657fa94b",
   "metadata": {},
   "source": [
    "-  LeNet-5 was designed primarily for handwritten digit recognition, particularly for recognizing characters in postal addresses.\n",
    "\n",
    "- The LeNet-5 architecture played a crucial role in demonstrating the effectiveness of CNNs for image recognition tasks. It showcased the benefits of convolutional layers for feature extraction and the use of pooling layers for downsampling and dimensionality reduction. \n",
    "\n",
    "- Although LeNet-5 was initially developed for handwritten digit recognition, its principles and structure have influenced the design of subsequent CNN architectures used in a wide range of computer vision applications, such as object recognition, image classification, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57048890-2d4f-4cb3-8cac-07c160c57d54",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c47c6-30bc-485b-82db-ce581bcd825f",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Describe the key concept of LetNet-5 and their respective purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a1d6f6-3464-437c-ab20-694ba6601f26",
   "metadata": {},
   "source": [
    "1. Convolutional Layers:\n",
    " - Purpose: Convolutional layers are responsible for detecting local features in the input image. They use small filters (also known as kernels) to slide over the input and perform convolution operations, capturing patterns like edges, corners, and textures.\n",
    " - Impact: These layers enable the network to learn hierarchical features by detecting basic edges and textures in the earlier layers and more complex combinations of features in deeper layers.\n",
    "\n",
    "2. Pooling (Subsampling) Layers:\n",
    " - Purpose: Pooling layers help downsample the feature maps produced by the convolutional layers. This reduces the spatial dimensions while retaining the most salient information. In LeNet-5, max pooling is used, where the maximum value within a small region is selected as the representative value for that region.\n",
    " - Impact: Pooling helps create translation invariance and reduces the sensitivity of the network to small changes in the input's position. It also reduces the computational load and the number of parameters in subsequent layers.\n",
    "\n",
    "3. Activation Functions:\n",
    " - Purpose: Activation functions introduce non-linearity to the network, enabling it to learn complex mappings from inputs to outputs. In LeNet-5, the sigmoid function is used as the activation function.\n",
    " - Impact: Activation functions allow the network to model more complex relationships between features and contribute to the network's ability to capture nonlinear patterns in data.\n",
    "\n",
    "4. Fully Connected Layers:\n",
    " - Purpose: Fully connected layers receive the features extracted by the convolutional and pooling layers and combine them to make final predictions. These layers are similar to the dense layers in traditional neural networks.\n",
    " - Impact: Fully connected layers enable the network to learn class-specific features by considering global context from the features extracted in earlier layers.\n",
    "\n",
    "5. Softmax Activation Function (Output Layer):\n",
    " - Purpose: The softmax activation function is applied in the output layer to convert the network's raw scores (logits) into class probabilities. It produces a normalized probability distribution over the possible classes.\n",
    " - Impact: The softmax function provides a way to interpret the network's output probabilistically, aiding in making class predictions.\n",
    "\n",
    "6. Gradient-Based Optimization:\n",
    " - Purpose: LeNet-5 introduced the concept of using gradient-based optimization algorithms (backpropagation) to train the network's weights. This enables the network to iteratively adjust its parameters to minimize the prediction error.\n",
    " - Impact: Gradient-based optimization is a fundamental concept in training deep neural networks. It allows the network to learn meaningful features and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775ba54-1cee-4937-a259-9f9c02ceeefe",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cac05-892f-413d-bc47-78ca17631154",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Discuss the advantages and limitations of LetNet-5 in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c79b3-4703-4a79-9923-ad1c53eba2dd",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Hierarchical Feature Learning: LeNet-5 demonstrated the power of learning hierarchical features through its convolutional layers. The architecture's ability to detect basic features in earlier layers and more complex patterns in deeper layers is crucial for image classification tasks.\n",
    "- Translation Invariance: The use of max pooling in LeNet-5 introduces translation invariance, making the network more robust to slight variations in object positions within the input images. This is beneficial for recognizing objects regardless of their positions.\n",
    "- Reduced Parameter Count: LeNet-5's architecture, particularly the pooling layers, leads to a reduction in the number of parameters compared to fully connected architectures. This helps prevent overfitting, especially when working with limited training data.\n",
    "- Localized Feature Detection: Convolutional layers in LeNet-5 are designed to detect localized features in the input image. This makes the network capable of identifying specific patterns in different areas of the image, which is crucial for accurate image classification.\n",
    "- Efficient Learning: The small filter sizes used in the convolutional layers of LeNet-5 allow the network to learn local features efficiently, reducing the need for large receptive fields and heavy computation.\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "- Limited Depth: LeNet-5 has a relatively shallow architecture compared to modern CNNs. Deeper architectures have shown the potential to learn more complex and abstract features, leading to better performance in challenging image classification tasks.\n",
    "- Small Input Size: LeNet-5 was designed to handle 32x32 grayscale images. While it was suitable for its time, modern datasets and applications often require processing larger and more high-resolution images.\n",
    "- Lack of Advanced Activation Functions: LeNet-5 uses the sigmoid activation function, which suffers from the vanishing gradient problem. More advanced activation functions like ReLU (Rectified Linear Unit) have been found to improve training stability and convergence.\n",
    "- No Batch Normalization or Regularization: LeNet-5 does not incorporate modern regularization techniques like dropout or batch normalization, which are effective for preventing overfitting and improving generalization. \n",
    "- Domain-Specific Design: LeNet-5 was primarily designed for handwritten digit recognition and may not perform optimally on more complex and diverse image classification tasks found in modern datasets like ImageNet.\n",
    "- Inadequate Capacity for Complex Tasks: While effective for digit recognition, LeNet-5's architecture may lack the capacity to handle the intricacies and variations present in more challenging image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4a58b-fce7-4396-afb9-20ecf3870301",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2922a0-775f-4198-b3c4-ee0c60bfaf85",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Implement LetNet-5 using a deep learning framework of your choice (eg., Tensoreflow, PyTorch) and train on public available dataset (e.g, MNIST). Evaluate its' performance and provide insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d4f0e9c-30f9-4f54-819e-9df21c4305c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb95e46f-1fa6-4bd9-bd54-cd2cd86ebd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "y_train = y_train / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0c475a9-2144-4295-b86b-29edcfc88cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96a7e2c1-896e-44b2-89f3-7dde72865096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "## Re-shape x_train and x_test\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert the target labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7a7ddfc-ef90-426c-be54-4f5094a6624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d_4 (Averag  (None, 12, 12, 6)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " average_pooling2d_5 (Averag  (None, 4, 4, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the Model Architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='relu', input_shape = (28, 28, 1)))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da3d7fea-d4c5-4b81-813d-d681c59eb767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 20s 42ms/step - loss: 0.0465 - accuracy: 0.9974 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 3.7308e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 3.7308e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 3.7308e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 3.7308e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 3.7308e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 17s 39ms/step - loss: 3.7087e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 3.7087e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 3.7087e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 17s 39ms/step - loss: 3.7087e-10 - accuracy: 1.0000 - val_loss: 2.5829e-10 - val_accuracy: 1.0000\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 8703.9707 - accuracy: 0.0980\n",
      "Test loss: 8703.9707, Test accuracy: 0.0980\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb724f-857f-44a2-9774-aa9593364f4d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4f38f-6067-4f16-b90c-c2490d53d14f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0041eab-69e1-4d87-8f21-cddeb43d392e",
   "metadata": {},
   "source": [
    "\n",
    "## TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8c668-5991-4e97-a910-9ba18ec64e6b",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Present an overview of AlexNet Architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ae726-20dd-4011-b84a-6b4cd9edd453",
   "metadata": {},
   "source": [
    "AlexNet has 8 layers, with 5 convolutional layers and 3 fully connected layers. The convolutional layers use rectified linear units (ReLU) as their activation function, which helped to improve the training speed and performance of the network. The fully connected layers use a softmax activation function to output a probability distribution.\n",
    "<br>\n",
    "\n",
    "Here is a more detailed overview of the AlexNet architecture with ILSVRC DataSet:\n",
    "- Input layer: The input layer is a 227x227x3 image.\n",
    "- Convolutional layer 1: This layer has 96 filters of size 11x11 with stride 4. The output of this layer is a 55x55x96 feature map.\n",
    "- Max pooling layer 1: This layer uses a 2x2 max pooling window with stride 2. The output of this layer is a 27x27x96 feature map.\n",
    "- Convolutional layer 2: This layer has 256 filters of size 5x5 with stride 1. The output of this layer is a 27x27x256 feature map.\n",
    "- Max pooling layer 2: This layer uses a 2x2 max pooling window with stride 2. The output of this layer is a 13x13x256 feature map.\n",
    "- Convolutional layer 3: This layer has 384 filters of size 3x3 with stride 1. The output of this layer is a 13x13x384 feature map.\n",
    "- Convolutional layer 4: This layer has 384 filters of size 3x3 with stride 1. The output of this layer is a 13x13x384 feature map.\n",
    "- Convolutional layer 5: This layer has 256 filters of size 3x3 with stride 1. The output of this layer is a 13x13x256 feature map.\n",
    "- Fully connected layer 1: This layer has 4096 neurons.\n",
    "- Fully connected layer 2: This layer has 4096 neurons.\n",
    "- Output layer: This layer has 1000 neurons, one for each class in the ILSVRC dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf420ab-a82e-49dc-aaa8-7039c0055c35",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c4d4e-5021-40dc-8aac-adbcdd4f0b77",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Explain the architectural inovations and introduced in AlexNet that contributed to it's breakthrough performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e54695-815e-4d42-82bb-0aec0246a314",
   "metadata": {},
   "source": [
    "- The use of rectified linear units (ReLU): ReLU is a non-linear activation function that helps to prevent the vanishing gradient problem, which can occur in deep neural networks. ReLU is much faster to compute than sigmoid or tanh activation functions, which also contributed to AlexNet's faster training speed.\n",
    "- The use of overlapping max pooling: Overlapping max pooling is a technique that helps to reduce the number of parameters in a CNN without sacrificing accuracy. AlexNet used overlapping max pooling windows with a stride of 2, which allowed it to learn more features from the input image while still keeping the number of parameters manageable.\n",
    "- The use of dropout: Dropout is a regularization technique that helps to prevent overfitting. AlexNet used dropout with a probability of 0.5, which meant that half of the neurons in each layer were randomly dropped out during training. This helped to prevent the network from learning too much from the training data and overfitting to it.\n",
    "- The use of data augmentation: Data augmentation is a technique that artificially increases the size of the training dataset by creating new training examples from the existing ones. AlexNet used data augmentation by randomly flipping, rotating, and translating the images in the training dataset. This helped to improve the generalization performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e21d90-ccf6-4df6-947c-d1b8ace3adb9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b200c-82f7-44c2-91ba-c622e8ba1d18",
   "metadata": {},
   "source": [
    " \n",
    "### 3. Discuss the role of convolutional layers, pooling layers and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2ec3a-45a3-40e6-a42b-b0a39c8599f1",
   "metadata": {},
   "source": [
    "- Convolutional layers are used to extract features from the input image. They do this by applying a filter to the image, which slides across the image and produces a new feature map. The filter is a small matrix of weights, and it is used to detect specific features in the image. For example, a filter might be used to detect edges, corners, or textures.\n",
    "- Pooling layers are used to reduce the size of the feature maps produced by the convolutional layers. This helps to reduce the number of parameters in the network, which makes it faster to train and easier to prevent overfitting. Pooling layers typically use a max pooling operation, which takes the maximum value from each region of the feature map.\n",
    "- Fully connected layers are used to classify the input image. They do this by taking the output of the pooling layers and connecting it to a large number of neurons. Each neuron in the fully connected layer represents a different class, and the output of the neuron is the probability that the input image belongs to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ae027-2116-4f63-8e9e-0195b5c0375f",
   "metadata": {},
   "source": [
    "The convolutional layers, pooling layers, and fully connected layers work together to extract features from the input image and classify it into one of 1000 classes. The convolutional layers extract low-level features, such as edges and corners. The pooling layers reduce the size of the feature maps produced by the convolutional layers, while preserving the most important features. The fully connected layers then classify the input image by taking the output of the pooling layers and connecting it to a large number of neurons, each of which represents a different class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed2c56-857d-488c-8110-1596bc3d4662",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec148a-0798-4fdf-9c2f-3bba93f2d4be",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Implement an AlexNet using DL framework of your choice and evaluate it's performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39499a4a-9880-487d-9e9b-5dfb49d35b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def alexnet(input_shape=(32, 32, 3)):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape),\n",
    "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "      \n",
    "      tf.keras.layers.Conv2D(192, (3, 3), padding='same', activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "      \n",
    "      tf.keras.layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "      tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "      tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "      \n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(4096, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(4096, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00c6baa6-9695-43ec-a571-e810f9cf5983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_76 (Conv2D)          (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_77 (Conv2D)          (None, 16, 16, 192)       110784    \n",
      "                                                                 \n",
      " max_pooling2d_43 (MaxPoolin  (None, 8, 8, 192)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_78 (Conv2D)          (None, 8, 8, 384)         663936    \n",
      "                                                                 \n",
      " conv2d_79 (Conv2D)          (None, 8, 8, 256)         884992    \n",
      "                                                                 \n",
      " conv2d_80 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_44 (MaxPoolin  (None, 4, 4, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,855,178\n",
      "Trainable params: 35,855,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = alexnet()\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf778c01-485d-4906-b160-9a4990f812dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "328cbbae-f812-42ea-9680-e45a6eb5443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "260cfa1c-2fb9-476b-9ed2-69e308e185b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "45000/45000 [==============================] - ETA: 0s - loss: 1.7008 - acc: 0.3664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mohan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 1207s 27ms/sample - loss: 1.7008 - acc: 0.3664 - val_loss: 1.4005 - val_acc: 0.4930\n",
      "Epoch 2/3\n",
      "45000/45000 [==============================] - 1207s 27ms/sample - loss: 1.3396 - acc: 0.5165 - val_loss: 1.2683 - val_acc: 0.5490\n",
      "Epoch 3/3\n",
      "45000/45000 [==============================] - 1039s 23ms/sample - loss: 1.1594 - acc: 0.5879 - val_loss: 1.0808 - val_acc: 0.6166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0896483503341674, 0.613]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model1.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model1.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee76d7-bc87-4afa-9886-c44b1a6c68b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
