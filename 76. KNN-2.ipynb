{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN lies in the way distances are calculated. \n",
    "- Euclidean distance is the straight-line distance between two points in space, while Manhattan distance is the sum of the absolute differences between corresponding coordinates. \n",
    "\n",
    "- Euclidean distance is sensitive to differences in all dimensions, whereas Manhattan distance is more influenced by differences in individual dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The optimal value of k in KNN can be selected through techniques such as cross-validation, grid search, or using algorithms like the elbow method.\n",
    "- Cross-validation helps assess performance across different k values, grid search systematically searches through a range of k values, and the elbow method observes how accuracy changes with increasing k to find an \"elbow\" point where accuracy plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The choice of distance metric significantly affects KNN performance. Euclidean distance is sensitive to outliers, while Manhattan distance is less affected. \n",
    "- Choose a distance metric based on the nature of the data and the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Common hyperparameters include the number of neighbors (k), the distance metric, and sometimes the weights assigned to neighbors. These parameters affect model behavior. \n",
    "- Tuning can involve adjusting k to balance bias-variance tradeoff, selecting an appropriate distance metric, and considering weighted distances based on proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The size of the training set can impact KNN performance. With small datasets, noise may heavily influence predictions, while large datasets may lead to increased computational costs. \n",
    "- Techniques like cross-validation can help estimate the optimal training set size for a balance between bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN can be sensitive to irrelevant or redundant features, and it may struggle with high-dimensional data. \n",
    "- Computationally, it can be expensive for large datasets. \n",
    "- Overcoming these drawbacks involves feature selection, dimensionality reduction, and considering alternative algorithms for efficiency, especially in cases with a large number of dimensions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
